{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader for FMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hari/anaconda3/envs/brain-diffuser/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, fmri_folder, images_folder, captions_file, transform=None):\n",
    "        self.fmri_files = [os.path.join(fmri_folder, file) for file in os.listdir(fmri_folder) if 'nsd_train_fmriavg_nsdgeneral' in file and 'batch' in file]\n",
    "        self.fmri_files = sorted(self.fmri_files, key=lambda x: int(x.split('_batch')[-1].split('.')[0]))[:-1]\n",
    "\n",
    "        self.images_files = [os.path.join(images_folder, file) for file in os.listdir(images_folder) if 'nsd_train_stim' in file and 'batch' in file]\n",
    "        self.images_files = sorted(self.images_files, key=lambda x: int(x.split('_batch')[-1].split('.')[0]))[:-1]\n",
    "        # self.captions = np.load(captions_file)\n",
    "        \n",
    "        # Assume all batch files have the same number of samples\n",
    "        self.samples_per_file = len(np.load(self.fmri_files[0]))\n",
    "        self.total_samples = self.samples_per_file * len(self.fmri_files)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        sample_idx = idx % self.samples_per_file\n",
    "\n",
    "        fmri = np.load(self.fmri_files[file_idx], mmap_mode='r')[sample_idx]\n",
    "        image = np.load(self.images_files[file_idx], mmap_mode='r')[sample_idx]\n",
    "        # caption = self.captions[idx % len(self.captions)]  # Cycle through captions if they are less than fmri and images\n",
    "\n",
    "        fmri = torch.from_numpy(fmri).float()\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = transforms.Resize((256,256))(image.permute(2,0,1))\n",
    "        return image, fmri\n",
    "    \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, fmri_file, images_file, captions_file):\n",
    "        self.fmri = np.load(fmri_file, mmap_mode='r')\n",
    "        self.images = np.load(images_file, mmap_mode='r')\n",
    "        # self.captions = np.load(captions_file, mmap_mode='r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fmri = torch.from_numpy(self.fmri[idx]).float()\n",
    "        image = torch.from_numpy(self.images[idx]).float()\n",
    "        # caption = self.captions[idx]\n",
    "\n",
    "        return image, fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "train_fmri_folder = 'data/processed_data/subj07'\n",
    "train_images_folder = 'data/processed_data/subj07'\n",
    "train_captions_file = 'data/processed_data/subj07/nsd_train_cap_sub7.npy'\n",
    "\n",
    "test_fmri_file = 'data/processed_data/subj07/nsd_test_fmriavg_nsdgeneral_sub7.npy'\n",
    "test_images_file = 'data/processed_data/subj07/nsd_test_stim_sub7.npy'\n",
    "test_captions_file = 'data/processed_data/subj07/nsd_test_cap_sub7.npy'\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images, fmri_data = zip(*batch)\n",
    "\n",
    "    # Convert images and fMRI data to tensors\n",
    "    images = torch.stack([torch.from_numpy(np.array(img)).float() for img in images])\n",
    "    fmri_data = torch.stack([torch.from_numpy(np.array(fmri)).float() for fmri in fmri_data])\n",
    "\n",
    "    # Handle captions as a list of strings\n",
    "    return images, fmri_data\n",
    "\n",
    "\n",
    "# Datasets\n",
    "train_dataset = CustomTrainDataset(train_fmri_folder, train_images_folder, train_captions_file, transform=True)\n",
    "test_dataset = CustomTestDataset(test_fmri_file, test_images_file, test_captions_file)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 10  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,  collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12682, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Intermediate layers\n",
    "        self.intermediate = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256 * 256 * 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.intermediate(x)  # Pass through the intermediate layers\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 3, 256, 256)  # Reshape to image dimensions\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 425, 425, 3)\n",
      "(50, 12682)\n"
     ]
    }
   ],
   "source": [
    "print(np.load(train_dataset.images_files[176]).shape)\n",
    "print(np.load(train_dataset.fmri_files[176]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/885 [00:00<?, ?batch/s]/tmp/ipykernel_412981/1444749204.py:32: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  fmri = torch.from_numpy(fmri).float()\n",
      "Epoch 1: 100%|██████████| 885/885 [01:31<00:00,  9.69batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Average Loss: 17657.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 885/885 [01:31<00:00,  9.72batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Average Loss: 17657.5143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 885/885 [01:30<00:00,  9.75batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 885/885 [01:31<00:00,  9.72batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 885/885 [01:31<00:00,  9.72batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 885/885 [01:30<00:00,  9.75batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 885/885 [01:31<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 885/885 [01:30<00:00,  9.75batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 885/885 [01:31<00:00,  9.72batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 885/885 [01:31<00:00,  9.71batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 885/885 [01:30<00:00,  9.73batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 885/885 [01:31<00:00,  9.72batch/s, loss=1.72e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 885/885 [01:30<00:00,  9.74batch/s, loss=1.72e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Average Loss: 17657.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "# if no GPU available, raise error\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise Exception('GPU not available')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = EncoderDecoder().to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# DataLoader setup (assuming you have already created train_loader)\n",
    "# train_loader = ...\n",
    "\n",
    "# Train for 3 epochs\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for images, fmri in tepoch:\n",
    "            images, fmri = images.to(device), fmri.to(device)\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(fmri)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            \n",
    "            # Clear memory\n",
    "            del images, fmri, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and state dict\n",
    "torch.save(model, 'entire_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
